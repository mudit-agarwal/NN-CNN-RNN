{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Overview : DL is a subset of ML and ML is a subset of AI\n",
    "\n",
    "Initally ML was a small subset of AI. That is AI used ML's algorithms for prediction. AI had other components like Robotics, etc.\n",
    "But today most of AI is ML. And most of ML is DL.\n",
    "\n",
    "That is nowadays we are getting best ML solutions by using Deep Learning. And hence DL is becoming increasingly important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Logistic Regression we were using the sigmoid function.The input we recieved was non-linear but we were predicting a linear O/P.(>0.5 - 1 and <0.5 - 0).\n",
    "\n",
    "This is not good as our output might be non-linear and we are predicting linear output.\n",
    "\n",
    "So we tried adding more features and also used SVM to get non-linear boundary, but even in that we were choosing which features to multiply and were manually choosing. This is computationaly unfeasible and overall inefficient as we want out model to choose the features on it's own and also predict a non-linear boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note : Neural Net helps us obtain non-linear decision boundaries by using non-linear intermediate functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Neural Net does is it uses multiple non-linear functions (for eg multiple Logistic Regressions) to act as an intermediate step.\n",
    "\n",
    "For ex, if we use a singlr Logistic Regression to find a non-linear decision boundary it's npt possible. So what NN does is, it combines multiple LR's and then combines them to get the final non-linear decision boundary O/P.\n",
    "\n",
    "Those multiple LR's are know as intermediate non-linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Input Layer - The Layer which contains the input's and the bias initially in the diagram 1.0\n",
    "\n",
    "2) Hidden Layers - All the middle non-linear intermediate functions in the image are known as the hidden layers\n",
    "\n",
    "3) Output Layers - The final O/P is know as the O/P layer\n",
    "\n",
    "4) Activation function - Each intermediate function can use a different function - for eg Sigmoid function,tanh,softmax,etc.\n",
    "\n",
    "5) Bias - It is that node which is always 1\n",
    "\n",
    "6) Weights - The line in between each node is known as weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Total Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Parameters = SUM ([left layer + 1] *[right layer])\n",
    "\n",
    "Keep on doing this for all layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Foward and Backward Propogation\n",
    "\n",
    "Foward - When you move from I/P layers to O/P layer to get an output is known as foward propogation\n",
    "\n",
    "Backward - When you move from O/P backwards to adjust the weights again and again so that you get a good ouput is known as backward propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement our own Neural Network\n",
    "For XOR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 2), (4, 1))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([[0,1,1,0]]).T\n",
    "X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivativeSig(z):\n",
    "    return sig(z)*(1 - sig(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hidden layer weight\n",
    "# Intially Random weights are taken. 2*x-1 is done to get these random values in between -1 and 1\n",
    "wh = 2 * np.random.random((2,2)) - 1\n",
    "bh = 2 * np.random.random((1,2)) - 1\n",
    "wo = 2 * np.random.random((2,1)) - 1\n",
    "bo = 2 * np.random.random((1,1)) - 1\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.05366889],\n",
       "        [0.93883161],\n",
       "        [0.95075096],\n",
       "        [0.04715312]]), array([[ 5.09055334,  5.18747132],\n",
       "        [-4.89517622, -5.30927736]]), array([[ 2.45925645, -2.88347892]]), array([[-7.12916923],\n",
       "        [ 7.46048312]]), array([[3.30265738]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Foward Propogation with 1 Hidden Layer\n",
    "\n",
    "for iter in range(10000):\n",
    "    output0 = X\n",
    "    inputHidden = np.dot(output0, wh) + bh\n",
    "    outputHidden = sig(inputHidden)\n",
    "    inputForOutputLayer = np.dot(outputHidden, wo) + bo\n",
    "    output = sig(inputForOutputLayer)\n",
    "    \n",
    "    first_term_output_layer = output - Y\n",
    "    second_term_output_layer = derivativeSig(inputForOutputLayer)\n",
    "    first_two_output_layer = first_term_output_layer * second_term_output_layer\n",
    "    \n",
    "    first_term_hidden_layer = np.dot(first_two_output_layer, wo.T)\n",
    "    second_term_hidden_layer = derivativeSig(inputHidden)\n",
    "    first_two_hidden_layer = first_term_hidden_layer * second_term_hidden_layer\n",
    "    \n",
    "    changes_output = np.dot(outputHidden.T, first_two_output_layer)\n",
    "    changes_output_bias = np.sum(first_two_output_layer, axis = 0, keepdims = True)\n",
    "    \n",
    "    changes_hidden = np.dot(output0.T, first_two_hidden_layer)\n",
    "    changes_hidden_bias = np.sum(first_two_hidden_layer, axis = 0, keepdims = True)\n",
    "    \n",
    "    wo = wo - lr*changes_output\n",
    "    bo = bo - lr*changes_output_bias\n",
    "    \n",
    "    wh = wh - lr*changes_hidden\n",
    "    bh = bh - lr*changes_hidden_bias\n",
    "    \n",
    "output0 = X\n",
    "inputHidden = np.dot(output0, wh) + bh\n",
    "outputHidden = sig(inputHidden)\n",
    "inputForOutputLayer = np.dot(outputHidden, wo) + bo\n",
    "output = sig(inputForOutputLayer)\n",
    "output, wh, bh, wo, bo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
